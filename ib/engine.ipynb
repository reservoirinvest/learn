{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring exec_pool as a core engine\n",
    "We are refactoring execution of a contract pool by building a new `async exec_pool` function.\n",
    "\n",
    "`async exec_pool`:\n",
    "1. processes sets of contracts to run specific algos \n",
    "2. with controlled concurrency \n",
    "3. with an option to produce df outputs\n",
    "   - which provides the capability to checkpoint to a pickle file...\n",
    "   - ... thereby `re-start` from near a point of failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKET = 'NSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import asyncio\n",
    "\n",
    "from ib_insync import IB, util, Option, MarketOrder\n",
    "from typing import Callable, Coroutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific to Jupyter. Will be ignored in IDE / command-lines\n",
    "import IPython as ipy\n",
    "if ipy.get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    util.startLoop()\n",
    "    pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get capability to import programs from `asyncib` folder\n",
    "cwd = pathlib.Path.cwd() # working directory from where python was initiated\n",
    "FSPATH = cwd.joinpath('data') # path to store data files\n",
    "LOGPATH = FSPATH # path to store log files\n",
    "\n",
    "IBPATH = cwd.parent.parent.joinpath('asyncib') # where ib programs are stored\n",
    "\n",
    "# append IBPATH to import programs.\n",
    "if str(IBPATH) not in sys.path:  # Convert it to string!\n",
    "    sys.path.append(str(IBPATH)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "from base import chains, unds, qualify, prices, margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the yaml config for HOST, PORT, CID\n",
    "with open(IBPATH.joinpath('var.yml')) as fi:\n",
    "    data = yaml.safe_load(fi)\n",
    "    \n",
    "HOST = data[\"COMMON\"][\"HOST\"]\n",
    "PORT = data[MARKET.upper()][\"PORT\"]\n",
    "CID = data[\"COMMON\"][\"CID\"]\n",
    "\n",
    "# Set log file\n",
    "util.logToFile(FSPATH.joinpath('./engine.log'), level=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ``async exec_pool`` algo\n",
    "### with concurrency control and post-processing checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def executeAsync(ib: IB(),\n",
    "                       algo: Callable[..., Coroutine],  # coro name\n",
    "                       cts: list, # list of contracts\n",
    "                       post_process: Callable[[set, pathlib.Path, str], pd.DataFrame]=None, # If checkpoint is needed\n",
    "                       FSPATH: pathlib.Path=None, # Necessary for post_process\n",
    "                       CONCURRENT: int=40, # adjust to prevent overflows\n",
    "                       TIMEOUT: None=None, # if None, no progress messages shown\n",
    "                       OP_FILENAME: str='', # output file name\n",
    "                       **kwargs, # keyword inputs for algo\n",
    "                       ):\n",
    "    \n",
    "    tasks = set()\n",
    "    results = set()\n",
    "    remaining = tuple(cts)\n",
    "    \n",
    "    # Determine unique names for tasks\n",
    "    try:\n",
    "        remaining[0].symbol\n",
    "        \n",
    "    except AttributeError: # It is a (contract, order) tuple for margin algo!\n",
    "        ct_name=\"c[0].symbol+c[0].lastTradeDateOrContractMonth[-4:]+c[0].right+str(c[0].strike)+'..'\"\n",
    "        \n",
    "    else: # for all algos, except margin algo\n",
    "        ct_name=\"c.symbol+c.lastTradeDateOrContractMonth[-4:]+c.right+str(c.strike)+'..'\"\n",
    "\n",
    "    # Get the results\n",
    "    while len(remaining):\n",
    "    \n",
    "        # Tasks limited by concurrency\n",
    "        if len(remaining) <= CONCURRENT:\n",
    "            tasks.update(asyncio.create_task(algo(ib, c, **kwargs), name=eval(ct_name)) for c in remaining)\n",
    "        else:\n",
    "            tasks.update(asyncio.create_task(algo(ib, c, **kwargs), name=eval(ct_name)) for c in list(remaining)[:CONCURRENT])\n",
    "\n",
    "        # Execute tasks\n",
    "        while len(tasks):\n",
    "\n",
    "            done, tasks = await asyncio.wait(tasks,\n",
    "                                            timeout=TIMEOUT,\n",
    "                                            return_when=asyncio.ALL_COMPLETED)\n",
    "\n",
    "            # Remove dones from remaining\n",
    "            done_names = [d.get_name() for d in done]\n",
    "            remaining = [c for c in remaining if eval(ct_name) not in done_names]\n",
    "            \n",
    "            # Update results and checkpoint\n",
    "            results.update(done)\n",
    "            \n",
    "            # Checkpoint the results\n",
    "            if post_process:\n",
    "                output = post_process(results, FSPATH, OP_FILENAME)\n",
    "            else:\n",
    "                output = results\n",
    "            \n",
    "            if TIMEOUT:\n",
    "                print(f'\\nCompleted {done_names[:2]} {len(results)} out of {len(cts)} .. remaining {[eval(ct_name) for c in remaining][:2]}')\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def save_df(results: set, FSPATH: pathlib.Path, file_name: str='') -> pd.DataFrame():\n",
    "\n",
    "    if results:\n",
    "        df = pd.concat([r.result() for r in results if r], ignore_index=True)\n",
    "        if file_name:\n",
    "            df.to_pickle(FSPATH.joinpath(file_name))\n",
    "    else:\n",
    "        df = pd.DataFrame([]) # results are not yet ready!\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing `async exec_pool` algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get symlots\n",
    "IBDATAPATH = IBPATH.joinpath('data', MARKET.lower())\n",
    "df_symlots = pd.read_pickle(IBDATAPATH.joinpath('df_symlots.pkl'))\n",
    "\n",
    "und_cts = df_symlots.contract\n",
    "\n",
    "### Uncomment for !!! DATA LIMITING underlying contracts\n",
    "# und_cts = df_symlots.contract[:50].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get underlyings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "with IB().connect(HOST, PORT, CID) as ib:\n",
    "    \n",
    "    # Get the underlyings\n",
    "    df_unds = ib.run(executeAsync(ib=ib, algo=unds, cts=und_cts, \n",
    "                                  CONCURRENT=40, TIMEOUT=2.0, \n",
    "                                  post_process=save_df, FSPATH=FSPATH, OP_FILENAME='df_unds.pkl',\n",
    "                                  **{'FILL_DELAY': 8},\n",
    "                                  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the chains"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Make the chains\n",
    "with IB().connect(HOST, PORT, CID) as ib:\n",
    "    df_chains = ib.run(executeAsync(ib=ib, algo=chains, cts=und_cts,\n",
    "                                  CONCURRENT=44, TIMEOUT=5,\n",
    "                                  post_process=save_df, FSPATH=FSPATH, OP_FILENAME='df_chains.pkl',\n",
    "                                  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualify ready-made options"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "opts = pd.read_pickle(IBDATAPATH.joinpath('df_qopts.pkl'))\n",
    "opts_cts = [Option(symbol=s, lastTradeDateOrContractMonth=e, \n",
    "                   strike=k, right=r, exchange='SMART', conId='') \n",
    "            for s, e, k, r, cid in zip(opts.symbol, opts.expiry, opts.strike, opts.right, range(1, len(opts)))]\n",
    "len(opts_cts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "with IB().connect(HOST, PORT, CID) as ib:\n",
    "    df_qopts = ib.run(executeAsync(ib=ib, algo=qualify, cts=opts_cts, \n",
    "                                  CONCURRENT=200, TIMEOUT=2.0,\n",
    "                                  post_process=save_df, FSPATH=FSPATH, OP_FILENAME='',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and qualify fresh set of options (MEGA)\n",
    "* Run this code if the ENTIRE set of options available in the market for ALL options"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Let us pick assemble the contracts to get our qualifications\n",
    "\n",
    "# Build the option contracts\n",
    "\n",
    "puts = [Option(s, e, k, 'P', 'SMART')\n",
    "        for s, e, k\n",
    "        in zip(df_chains.symbol, df_chains.expiry, df_chains.strike)]\n",
    "\n",
    "calls = [Option(s, e, k, 'C', 'SMART')\n",
    "        for s, e, k\n",
    "        in zip(df_chains.symbol, df_chains.expiry, df_chains.strike)]\n",
    "\n",
    "cts = puts + calls\n",
    "\n",
    "df_cts = util.df(cts).iloc[:, :6].\\\n",
    "            rename(columns={'lastTradeDateOrContractMonth': 'expiry'}).\\\n",
    "                assign(contract=cts)\n",
    "                \n",
    "df_cts.conId = None # Replace `conId` as None to track completeness\n",
    "\n",
    "df_cts = df_cts.sample(len(df_cts)) # !!! TO BE DELETED in live run\n",
    "df_cts = df_cts.reset_index(drop=True) # Index used to track progress\n",
    "\n",
    "# *** Done once only in the TEST ***!!!\n",
    "df_cts.to_pickle(FSPATH.joinpath('all_raw_opts.pkl'))\n",
    "\n",
    "df_cts = pd.read_pickle(FSPATH.joinpath('all_raw_opts.pkl'))\n",
    "\n",
    "s = sorted(df_cts.symbol.unique())\n",
    "print(f\"# of symbols: {len(s)}, # of option contracts: {len(df_cts)}, # no of expiries: {len(df_cts.expiry.unique())}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "fresh_opts = df_cts.contract.sample(500).to_list() # !!! DATA LIMITER - 500 contracts\n",
    "with IB().connect(HOST, PORT, CID) as ib:\n",
    "    df_qopts = ib.run(executeAsync(ib=ib, algo=qualify, cts=fresh_opts, \n",
    "                                  CONCURRENT=200, TIMEOUT=5.0,\n",
    "                                  post_process=save_df, FSPATH=FSPATH, OP_FILENAME='df_qualopts1.pkl',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the price of qualified options"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "df_qopts = pd.read_pickle(FSPATH.joinpath('df_qualopts1.pkl'))\n",
    "\n",
    "with IB().connect(HOST, PORT, CID) as ib:\n",
    "    df_price = ib.run(executeAsync(ib=ib, algo=prices, cts=df_qopts.contract.to_list(), \n",
    "                                  CONCURRENT=200, TIMEOUT=10.0,\n",
    "                                  post_process=save_df, FSPATH=FSPATH, OP_FILENAME='',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare cos for margins from qualified options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_symlots = pd.read_pickle(IBDATAPATH.joinpath('df_symlots.pkl'))\n",
    "df_raw_opts = pd.read_pickle(IBDATAPATH.joinpath('df_qopts.pkl'))\n",
    "\n",
    "if MARKET == 'NSE':\n",
    "    df_raw_opts['expiryM'] = df_raw_opts.expiry.apply(\n",
    "        lambda d: d[:4] + '-' + d[4:6])\n",
    "    cols1 = ['symbol', 'expiryM']\n",
    "    df_raw_opts = df_raw_opts.set_index(cols1).join(\n",
    "        df_symlots[cols1 + ['lot']].set_index(cols1)).reset_index()\n",
    "    df_raw_opts = df_raw_opts.drop('expiryM', 1)\n",
    "else:\n",
    "    df_raw_opts['lot'] = 100\n",
    "\n",
    "# ... build cos (contract, orders)\n",
    "opts = df_raw_opts.contract.to_list()\n",
    "orders = [MarketOrder('SELL', lot / lot) if MARKET.upper() ==\n",
    "          'SNP' else MarketOrder('SELL', lot) for lot in df_raw_opts.lot]\n",
    "cos = [(c, o) for c, o in zip(opts, orders)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos = cos[:500] # !!! DATA LIMITER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get option margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with IB().connect(HOST, PORT, CID) as ib:\n",
    "    df_margins = ib.run(executeAsync(ib=ib, algo=margins, cts=cos, \n",
    "                                  CONCURRENT=200, TIMEOUT=5.0,\n",
    "                                  post_process=save_df, FSPATH=FSPATH, OP_FILENAME='df_margins.pkl',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get option prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with IB().connect(HOST, PORT, CID) as ib:\n",
    "    df_price = ib.run(executeAsync(ib=ib, algo=prices, cts=opts, \n",
    "                                  CONCURRENT=200, TIMEOUT=10.0,\n",
    "                                  post_process=save_df, FSPATH=FSPATH, OP_FILENAME='df_optprices.pkl',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_price[~df_price.price.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.set_index('conId').join(df_margins[['conId', 'margin', 'lot', 'comm']].set_index('conId')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[~df1.iv.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
