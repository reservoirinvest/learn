{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38132bit2115ca79f6634adbad3a74c57c1d7c04",
   "display_name": "Python 3.8.1 32-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview\n",
    "\n",
    "Ref: [Calculating centred and non-centred volatility](http://vixandmore.blogspot.com/2009/12/calculating-centered-and-non-centered.html)\n",
    "\n",
    "Stock Volatility measures how much a stock tends to move. There are many ways to calculate volatility, for e.g. through:\n",
    "\n",
    "- Daily/Weekly/Monthly range\n",
    "- Average True Range\n",
    "- Standard Deviation\n",
    "\n",
    "Standard Deviation is the most popular way to measure volatility. Standard Deviation for stock price can be computed in multiple ways. Let us go through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# computing SD of a series of values\n",
    "data = [161.47, 159.86, 159.27, 159.98, 159.78, 157.21, 157.5, 157.86, 160.95, 161.6, 159.85, \n",
    "         157.48, 155.32, 160.43, 159.45, 158.19, 155.78, 154.96, 156.53, 149.46, 148.15] # gives sd of 1.59%\n",
    "\n",
    "s = pd.Series(data, name='data')\n",
    "df = pd.DataFrame(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) From math library's standard deviation computation.\n",
    "\n",
    "The code for this is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = df.data.std()\n",
    "amean = df.data.mean()\n",
    "dict(asd=asd, amean=amean)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arithmetic standard deviation makes sense for a stable mean. However, in stock prices the price keeps current price keeps changing. So it is better to use **geometric standard deviation** or GSD. \n",
    "\n",
    "Here are different ways of getting GSD:\n",
    "\n",
    "## 2a) From daily log returns assuming zero mean and averaging squares\n",
    "Ref: [this Quora link](https://www.quora.com/How-do-you-calculate-the-standard-deviation-for-a-stock)\n",
    "\n",
    "Formula for geometric standard deviation is represented by:\n",
    "     \n",
    "gsd1 = $\\exp{\\Big(\\sqrt{{\\frac{\\sum_{1}^n (\\ln\\frac{x[n]}{x[n-1]})^{2}}{n}}}}\\Big)$\n",
    "\n",
    "Geometric standard deviation is computed over geometric mean, which is represented by the formula:\n",
    "\n",
    "gm = $\\left (\\prod_{a=1}^{b}x_i  \\right )^{\\frac{1}{n}} = \\sqrt[n]{x_1x_2...x_n}$\n",
    "\n",
    "The upper and lower values of 1 GSD are computed by dividing and multiplying geometric mean with the geometric standard deviation\n",
    "\n",
    "The code for these are as follows:\n",
    "\n",
    "```\n",
    "import math\n",
    "from scipy.stats.mstats import gmean\n",
    "sdmult = 2 # no of standard deviations\n",
    "gsd1 = math.exp(math.sqrt(df.data.rolling(2).apply(lambda x: math.log(x[0]/x[-1])**2, raw=True).mean()))\n",
    "gm = gmean(df)\n",
    "glo1 = gm*(1-(gm-gm/gsd1)/gm*sdmult) # gm/gsd1 ... but taking sdmult into consideration\n",
    "ghi1 = gm*(1+(gm*gsd1-gm)/gm*sdmult) # gm*gsd1 ... but taking sdmult into consideration\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats.mstats import gmean\n",
    "sdmult = 2 # no of standard deviations\n",
    "gsd1 = math.exp(math.sqrt(df.data.rolling(2).apply(lambda x: math.log(x[0]/x[-1])**2, raw=True).mean()))\n",
    "gm = gmean(df)\n",
    "glo1 = gm*(1-(gm-gm/gsd1)/gm*sdmult) # gm/gsd1 ... but taking sdmult into consideration\n",
    "ghi1 = gm*(1+(gm*gsd1-gm)/gm*sdmult) # gm*gsd1 ... but taking sdmult into consideration\n",
    "\n",
    "dict(gm=gm, gsd1=gsd1, glo1=glo1, ghi1=ghi1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2b) From daily log returns without squaring and square-rooting\n",
    "\n",
    "The formula for this is:\n",
    "\n",
    "sd2 = ${\\exp(\\sigma\\Big({{\\ln(\\frac{x[n]}{x[n-1]})}}\\Big))}$\n",
    "\n",
    "This equation gives a slightly lesser band than taking rooting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats.mstats import gmean\n",
    "sdmult = 2 # no of standard deviations\n",
    "gsd2 = math.exp(df.data.rolling(2).apply(lambda x: math.log(x[0]/x[-1]), raw=True).std())\n",
    "glo2 = gm*(1-(gm-gm/gsd2)/gm*sdmult) # gm/gsd2 ... but taking sdmult into consideration\n",
    "ghi2 = gm*(1+(gm*gsd2-gm)/gm*sdmult) # gm*gsd2 ... but taking sdmult into consideration\n",
    "\n",
    "dict(gm=gm, gsd2=gsd2, glo2=glo2, ghi2=ghi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2c) Centered Historical Volatiity using look-back (Bill Luby)\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. Select a desired lookback period in trading days (lookback period)\n",
    "2. Gather closing prices for the full lookback period, plus one additional day (lookback +1)\n",
    "4. Calculate the daily close-to-close price changes in a security for each day in the lookback period (daily change)\n",
    "5. Determine the natural log of each daily percentage change (log of daily changes)\n",
    "6. Calculate the mean of all the natural logs of the closing prices for the lookback period (log lookback mean)\n",
    "7. For each day, subtract the lookback mean from the log of daily changes (daily difference)\n",
    "8. Square all the differences between the mean and the daily change (daily variance)\n",
    "9. Sum all the squares of the differences (sum of variances)\n",
    "10. Divide the sum of the squares of the variances by the lookback period (lookback variance)\n",
    "11. Take the square root of the lookback variance (historical volatility, expressed as a standard deviation)\n",
    "12. To convert this to an annual volatility percent, take HV expressed as standard deviation and multiply it by square root of no of trading days in a year (252) and then by 100.\n",
    "13. Compute the high and low bands for the lookback period\n",
    "\n",
    "## 2d) Non-centered Historical volatility on a lookback (Bill Luby - faster)\n",
    "\n",
    "The previous calculation reflects a centred approach where daily price changes are characterized relative to a mean value for the entire period. Instead of that one could assume that, in the long run, the mean change in price approaches zero and hence not meamingful. So, if the mean is not meaningful, we can dump it and not subtract it from the daily changes, so all calculations involving the mean can be dropped! This non-centred approach for historical volaility is sometimes called \"ditching the mean\".\n",
    "\n",
    "This method is shorter and [gives better numbers to traders.](http://vixandmore.blogspot.com/2009/12/calculating-centered-and-non-centered.html). \n",
    "\n",
    "In this calculation the following steps are taken:\n",
    "\n",
    "1. Select a desired (10-day) lookback period in trading days (lookback period)\n",
    "2. Gather closing prices for the full lookback period, plus one additional day (lookback +1)\n",
    "4. Calculate the daily close-to-close price changes in a security for each day in the lookback period (daily change)\n",
    "5. Determine the natural log of each daily percentage change (log of daily changes)\n",
    "6. Determine the historical volatility by multiplying the Standard Deviation of the log change with root of 252 and multiplying by 100.\n",
    "\n",
    "Let us now see how all of the above work on a pandas ohlc data-set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "MARKET = 'nse'\n",
    "TRADING_DAYS=252\n",
    "STDMULT = 2\n",
    "\n",
    "oldp = str(Path(os.getcwd()).parents[3])+f\"\\ibkr\\data\\{MARKET.lower()}\"\n",
    "newp = str(Path(os.getcwd()).parents[0])+f\"\\ib\\data\\{MARKET.lower()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_ohlc = pd.read_pickle(newp+'\\\\_df_ohlc.pkl')\n",
    "df_und = pd.read_pickle(newp+'\\\\_df_und.pkl')\n",
    "\n",
    "# pick up NIFTY50 only and reset its index\n",
    "cols = ['symbol', 'dte', 'date', 'close', 'rise', 'fall'] # interested columns\n",
    "df = df_all_ohlc[df_all_ohlc.symbol=='NIFTY50']\n",
    "\n",
    "# extract from df_und data\n",
    "df = df.set_index('symbol').join(df_und[['symbol', 'undPrice', 'iv_ib', 'hv_ib', 'avg_div']].set_index('symbol')).reset_index()\n",
    "\n",
    "# replace iv_ib and hv_ib as per dte\n",
    "df = df.assign(\n",
    "    iv_ib=df.iv_ib*[math.sqrt(j/365) for j in df.index],\n",
    "    hv_ib=df.hv_ib*[math.sqrt(j/365) for j in df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are the comparisons of various algorithms for hv.\n",
    "\n",
    "### 1) Luby's formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hv_luby(df):\n",
    "    \"\"\"Computes historical volatility based on Bill Luby's formula\n",
    "    Ref: http://vixandmore.blogspot.com/2009/12/calculating-centered-and-non-centered.html\n",
    "    Arg: df as dataframe for a symbol with close field\n",
    "    Outputs: historical volaility percentage\n",
    "    \"\"\"\n",
    "    # Determine the natural log of each daily percentage change (log of daily changes)\n",
    "    log_of_changes = df.close.rolling(2).apply(\n",
    "        func=lambda x: math.log(x[1]/x[0]),\n",
    "        raw=True)\n",
    "\n",
    "    # Calculate the mean of all the natural logs of the closing prices for the lookback period (log lookback mean)\n",
    "    lookback_mean = log_of_changes.expanding().mean()\n",
    "\n",
    "    # For each day, subtract the lookback mean from the log of daily changes (daily difference)\n",
    "    daily_difference = log_of_changes - lookback_mean\n",
    "\n",
    "    # Square all the differences between the mean and the daily change (daily variance)\n",
    "    daily_variance = daily_difference**2\n",
    "\n",
    "    # Sum all the squares of the differences (sum of variances)\n",
    "    sum_of_variances = daily_variance.expanding().sum()\n",
    "\n",
    "    # Divide the sum of the squares of the variances by the lookback period (lookback variance)\n",
    "    lookback_variance = sum_of_variances/sum_of_variances.index\n",
    "\n",
    "    # Take the square root of the lookback variance (historical volatility, expressed as a standard deviation)\n",
    "    hv_as_sd = lookback_variance.apply(lambda x: math.sqrt(x))\n",
    "\n",
    "    # Compute annual volatility by multiplying by root of 252\n",
    "    hv_luby = hv_as_sd.apply(lambda x: x*math.sqrt(TRADING_DAYS))\n",
    "\n",
    "    df = df.assign(hv_luby=hv_luby)\n",
    "\n",
    "    return df\n",
    "\n",
    "df  = hv_luby(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2) TastyTrades formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hv_tw(df):\n",
    "    \"\"\"Historical Volatility by Tasty Works\n",
    "    Ref: https://youtu.be/omVKR85pw2s\n",
    "    Arg: df as dataframe for a symbol with close field\n",
    "    Outputs: historical volaility percentage\n",
    "    Note: To compute price bands\n",
    "    Upper Band = price*(1 + ln_avg + STDMULT*ln_stdev)\n",
    "    Lower Band = price*(1 + ln_avg - STDMULT*ln_stdev)\n",
    "    \"\"\"\n",
    "    # Log-normal of price ratio\n",
    "    ln = df.close.rolling(2).apply(\n",
    "            lambda x: math.log(x[1]/x[0]))\n",
    "\n",
    "    # stdev and mean of ln\n",
    "    ln_avg = ln.expanding().mean()\n",
    "    ln_stdev = ln.expanding().std(ddof=1)\n",
    "\n",
    "    df = df.assign(ln_avg=ln_avg, ln_stdev=ln_stdev)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = hv_tw(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all of them together\n",
    "\n",
    "up_tw = pd.Series(df.undPrice*(1+df.ln_avg+STDMULT*df.ln_stdev), name='up_tw')\n",
    "lo_tw = pd.Series(df.undPrice*(1+df.ln_avg-STDMULT*df.ln_stdev), name='lo_tw')\n",
    "up_luby = pd.Series(df.undPrice*(1+STDMULT*df.hv_luby), name='up_luby')\n",
    "lo_luby = pd.Series(df.undPrice*(1-STDMULT*df.hv_luby), name='lo_luby')\n",
    "up_iv_ib = pd.Series(df.undPrice*(1+STDMULT*df.iv_ib), name='up_iv_ib')\n",
    "lo_iv_ib = pd.Series(df.undPrice*(1-STDMULT*df.iv_ib), name='lo_iv_ib')\n",
    "up_hv_ib = pd.Series(df.undPrice*(1+STDMULT*df.hv_ib), name='up_hv_ib')\n",
    "lo_hv_ib = pd.Series(df.undPrice*(1-STDMULT*df.hv_ib), name='lo_hv_ib')\n",
    "\n",
    "df=df.assign(up_tw=up_tw, lo_tw=lo_tw, up_luby=up_luby, lo_luby=lo_luby, up_iv_ib=up_iv_ib, lo_iv_ib=lo_iv_ib, up_hv_ib=up_hv_ib, lo_hv_ib=lo_hv_ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.dte == 312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "# Get square of log ratios\n",
    "log_square = df.close.rolling(2).apply(\n",
    "    lambda x: math.log(x[0]/x[-1])**2,\n",
    "    raw=True)\n",
    "\n",
    "# Sum the squares\n",
    "sum_log_square = log_square.expanding().sum()\n",
    "\n",
    "# Divide the sum with number of observations\n",
    "avg_sum_log_square = sum_log_square/sum_log_square.index\n",
    "\n",
    "# get the root of avg sum log square\n",
    "gsd_fact = avg_sum_log_square.apply(math.sqrt).apply(math.exp)-1\n",
    "\n",
    "# get the geometric mean\n",
    "gm = df.close.expanding().apply(gmean)\n",
    "\n",
    "# get the geometric mean difference from undPrice\n",
    "gm_fact = 1-gm/df.undPrice.unique()\n",
    "\n",
    "df = df.assign(gm_fact = gm_fact, gsd_fact = gsd_fact)\n",
    "\n",
    "# what we do with these gm and gsdd factors are unclear, especially with negative gm_facts\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<em>Note</em>: The following cells are work-in-progress and have been temporarily abandoned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was made to test and build a case for computation with geometric mean\n",
    "# Failed Again\n",
    "# Refer to https://stats.stackexchange.com/questions/449482/geometric-standard-deviation-on-a-shifted-mean for stackoverflow question raised\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import gstd\n",
    "\n",
    "np.random.seed(999)\n",
    "\n",
    "bmark = 10 # benchmark\n",
    "sdmult = 1 # standard deviation multiple\n",
    "\n",
    "data = pd.Series(np.random.randint(8, high=12, size=100), name='Value')\n",
    "\n",
    "am = pd.Series(data.expanding().mean(), name=\"aMean\") # arithmetic mean\n",
    "asd = pd.Series(data.expanding().std(ddof=1), name=\"aSD\") # arithmetic standard deviation\n",
    "a_abv = pd.Series(bmark-(bmark-am)+sdmult*asd, name=\"a_abv\") # benchmark value above multiple of sd\n",
    "a_blw = pd.Series(bmark-(bmark-am)-sdmult*asd, name=\"a_blw\") # benchmark value below multiple of sd\n",
    "\n",
    "gm_fact = pd.Series(data.expanding().apply(gmean), name=\"gMean\") # geometric mean factor\n",
    "gsd_fact = pd.Series(data.apply(math.log).expanding().apply(np.std).apply(math.exp), name=\"gSD\") # geometric standard deviation factor\n",
    "\n",
    "g_ucl = pd.Series((bmark-(bmark-gm_fact))*(math.sqrt(sdmult)+gsd_fact), name=\"g_ucl\") # geometric upper control limit\n",
    "g_lcl = pd.Series((bmark-(bmark-gm_fact))/(math.sqrt(sdmult)+gsd_fact), name=\"g_lcl\") # geometric lower control limit\n",
    "\n",
    "# computing above and below benchmark\n",
    "pd.DataFrame([data, am, asd, a_abv, a_blw, gm_fact, gsd_fact, g_ucl, g_lcl]).T"
   ]
  }
 ]
}